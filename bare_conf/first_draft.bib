
@article{noauthor_multichannel_nodate,
	title = {Multichannel {Signal} {Processing} with {Deep} {NeuralNetworks} for {Automatic} {Speech} {Recognition}},
	url = {http://www.cs.cmu.edu/~chanwook/MyPapers/t_sainath_ieee_acm_trans_2017.pdf}
}

@misc{noauthor_scipy.signal.cwt_nodate,
	title = {scipy.signal.cwt — {SciPy} v0.15.1 {Reference} {Guide}},
	url = {https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.signal.cwt.html},
	urldate = {2019-03-01}
}

@misc{noauthor_26._nodate,
	title = {26. {Continuous} {Wavelet} {Transform} — {ObsPy} {Documentation} (1.1.1)},
	url = {https://docs.obspy.org/tutorial/code_snippets/continuous_wavelet_transform.html},
	urldate = {2019-03-01}
}

@misc{noauthor_application_nodate,
	title = {Application of {Wavelet} {Analysis} in {EMG} {Feature} {Extraction} - {Google} {Search}},
	url = {https://www.google.com/search?safe=off&ei=2H54XL2IL4fF-gTusYWQDw&q=Application+of+Wavelet+Analysis+in+EMG+Feature+Extraction&oq=Application+of+Wavelet+Analysis+in+EMG+Feature+Extraction&gs_l=psy-ab.3..0.15817.15817..16229...0.0..0.99.99.1......0....2j1..gws-wiz.DZS21dgfEjo},
	urldate = {2019-03-01}
}

@misc{noauthor_continuous_nodate,
	title = {Continuous {Wavelet} {Transform} ({CWT})},
	url = {https://www.weisang.com/en/documentation/timefreqspectrumalgorithmscwt_en/#},
	abstract = {Continuous Wavelet Transform (CWT) Continuous Wavelet Transform (CWT) {\textless}{\textless} Klicken, um Inhaltsverzeichnis anzuzeigen {\textgreater}{\textgreater}   Continuous Wavelet Transform (CWT) The Continuous Wavelet Transform (CWT) is used to decompose a signal into wavelets. Wavelets are small oscillations that are highly localized in time. While the Fourier Transform decomposes a signal into infinite length sines and … Continued},
	language = {en-US},
	urldate = {2019-03-01},
	journal = {weisang.com},
	file = {Snapshot:/home/rommel/Zotero/storage/2U9S4N6Z/Continuous Wavelet Transform (CWT).html:text/html}
}

@article{strazza_time-frequency_2017,
	title = {Time-frequency analysis of surface {EMG} signals for maximum energy localization during walking},
	url = {https://link.springer.com/chapter/10.1007/978-981-10-5122-7_124},
	doi = {10.1007/978-981-10-5122-7_124},
	abstract = {The purpose of this work is to assess the maximum energy localization in time-frequency domain of the surface EMG signal of the main lower-limb muscles usually involved in able-bodied walking. The...},
	language = {en},
	urldate = {2019-03-01},
	journal = {EMBEC \& NBC 2017},
	author = {Strazza, Annachiara and Verdini, Federica and Burattini, Laura and Fioretti, Sandro and Nardo, Francesco Di},
	month = jun,
	year = {2017},
	pages = {494--497},
	file = {Snapshot:/home/rommel/Zotero/storage/DJCKPCW3/Strazza et al. - 2017 - Time-frequency analysis of surface EMG signals for.html:text/html}
}

@inproceedings{lopez-larraz_syllable-based_2010,
	title = {Syllable-based speech recognition using {EMG}},
	doi = {10.1109/IEMBS.2010.5626426},
	abstract = {This paper presents a silent-speech interface based on electromyographic (EMG) signals recorded in the facial muscles. The distinctive feature of this system is that it is based on the recognition of syllables instead of phonemes or words, which is a compromise between both approaches with advantages as (a) clear delimitation and identification inside a word, and (b) reduced set of classification groups. This system transforms the EMG signals into robust-in-time feature vectors and uses them to train a boosting classifier. Experimental results demonstrated the effectiveness of our approach in three subjects, providing a mean classification rate of almost 70\% (among 30 syllables).},
	booktitle = {2010 {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology}},
	author = {Lopez-Larraz, E. and Mozos, O. M. and Antelis, J. M. and Minguez, J.},
	month = aug,
	year = {2010},
	keywords = {EMG, feature extraction, boosting classifier, Decision trees, Electrodes, electromyographic signals, electromyography, Electromyography, facial muscles, Facial muscles, Facial Muscles, medical signal processing, Muscles, Natural Language Processing, Pattern Recognition, Automated, robust-in-time feature vectors, Semantics, signal classification, silent-speech interface, Speech, Speech Production Measurement, speech recognition, Speech recognition, Speech Recognition Software, syllable-based speech recognition},
	pages = {4699--4702},
	file = {IEEE Xplore Abstract Record:/home/rommel/Zotero/storage/J724L9CX/5626426.html:text/html;IEEE Xplore Full Text PDF:/home/rommel/Zotero/storage/MNRI8H26/Lopez-Larraz et al. - 2010 - Syllable-based speech recognition using EMG.pdf:application/pdf}
}

@article{wang_deep_2017,
	title = {Deep {Learning} for {Sensor}-based {Activity} {Recognition}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {Sensor}-based {Activity} {Recognition}},
	url = {https://arxiv.org/abs/1707.03502v2},
	doi = {10.1016/j.patrec.2018.02.010},
	abstract = {Sensor-based activity recognition seeks the profound high-level knowledge
about human activities from multitudes of low-level sensor readings.
Conventional pattern recognition approaches have made tremendous progress in
the past years. However, those methods often heavily rely on heuristic
hand-crafted feature extraction, which could hinder their generalization
performance. Additionally, existing methods are undermined for unsupervised and
incremental learning tasks. Recently, the recent advancement of deep learning
makes it possible to perform automatic high-level feature extraction thus
achieves promising performance in many areas. Since then, deep learning based
methods have been widely adopted for the sensor-based activity recognition
tasks. This paper surveys the recent advance of deep learning based
sensor-based activity recognition. We summarize existing literature from three
aspects: sensor modality, deep model, and application. We also present detailed
insights on existing work and propose grand challenges for future research.},
	language = {en},
	urldate = {2019-02-27},
	author = {Wang, Jindong and Chen, Yiqiang and Hao, Shuji and Peng, Xiaohui and Hu, Lisha},
	month = jul,
	year = {2017},
	file = {Full Text PDF:/home/rommel/Zotero/storage/AJTP7GLS/Wang et al. - 2017 - Deep Learning for Sensor-based Activity Recognitio.pdf:application/pdf;Snapshot:/home/rommel/Zotero/storage/N39Y2LLP/1707.html:text/html}
}

@misc{noauthor_statistics_2015,
	title = {Statistics on {Voice}, {Speech}, and {Language}},
	url = {https://www.nidcd.nih.gov/health/statistics/statistics-voice-speech-and-language},
	abstract = {On this page: Voice Speech Language Books and Articles More Information The functions, skills and abilities of voice, speech, and language are related. Some dictionaries and textbooks use the terms almost interchangeably. But, for scientists and medical professionals, it is important to distinguish among them.},
	language = {en},
	urldate = {2019-02-27},
	journal = {NIDCD},
	month = aug,
	year = {2015},
	file = {Snapshot:/home/rommel/Zotero/storage/HVW388TF/statistics-voice-speech-and-language.html:text/html}
}

@article{graves_speech_2013,
	title = {Speech {Recognition} with {Deep} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1303.5778},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classiﬁcation make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the ﬂexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we ﬁnd that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	language = {en},
	urldate = {2019-02-27},
	journal = {arXiv:1303.5778 [cs]},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	month = mar,
	year = {2013},
	note = {arXiv: 1303.5778},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: To appear in ICASSP 2013},
	file = {Graves et al. - 2013 - Speech Recognition with Deep Recurrent Neural Netw.pdf:/home/rommel/Zotero/storage/FZFNN499/Graves et al. - 2013 - Speech Recognition with Deep Recurrent Neural Netw.pdf:application/pdf}
}

@inproceedings{wand_pattern_2014,
	address = {Chicago, IL},
	title = {Pattern learning with deep neural networks in {EMG}-based speech recognition},
	isbn = {978-1-4244-7929-0},
	url = {http://ieeexplore.ieee.org/document/6944550/},
	doi = {10.1109/EMBC.2014.6944550},
	abstract = {We report on classiﬁcation of phones and phonetic features from facial electromyographic (EMG) data, within the context of our EMG-based Silent Speech interface. In this paper we show that a Deep Neural Network can be used to perform this classiﬁcation task, yielding a signiﬁcant improvement over conventional Gaussian Mixture models. Our central contribution is the visualization of patterns which are learned by the neural network. With increasing network depth, these patterns represent more and more intricate electromyographic activity.},
	language = {en},
	urldate = {2019-02-27},
	booktitle = {2014 36th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society}},
	publisher = {IEEE},
	author = {Wand, Michael and Schultz, Tanja},
	month = aug,
	year = {2014},
	pages = {4200--4203},
	annote = {
25 sessions from 20 speakers, eachcomprising 200 read English-language utterances spoken innormal, audible speech.
This data was recorded in bipolar fashion, where the difference of twoadjacent channels is taken to reduce common mode artifacts,thus we finally got 35 (5 7) EMG channels. Sampling wasperformed at 2048Hz.
We assumethe input data to be Gaussian distributed and modify theRBM algorithm to get a Gaussian-Bernoulli RBM [14]. Ourcode is based on the original scripts by Hinton
Uses GMM and Restricted Boltzmann Machine Algorithm, not LSTM  or RNN or Convolutional neural networks.
},
	file = {Wand and Schultz - 2014 - Pattern learning with deep neural networks in EMG-.pdf:/home/rommel/Zotero/storage/ZYPKJZEN/Wand and Schultz - 2014 - Pattern learning with deep neural networks in EMG-.pdf:application/pdf}
}

@article{karpathy_visualizing_2016,
	title = {{VISUALIZING} {AND} {UNDERSTANDING} {RECURRENT} {NETWORKS}},
	abstract = {Recurrent Neural Networks (RNNs), and speciﬁcally a variant with Long ShortTerm Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with ﬁnite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
	language = {en},
	author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
	year = {2016},
	pages = {11},
	file = {Karpathy et al. - 2016 - VISUALIZING AND UNDERSTANDING RECURRENT NETWORKS.pdf:/home/rommel/Zotero/storage/Z9YLK2JS/Karpathy et al. - 2016 - VISUALIZING AND UNDERSTANDING RECURRENT NETWORKS.pdf:application/pdf}
}

@article{pauk_419._2008,
	title = {419. {Different} techniques for {EMG} signal processing},
	volume = {10},
	abstract = {Electromyography signal can be used for biomedical applications. It is complicated in interpretation, so it acquires advanced methods for detection, decomposition, processing, and classification. The techniques of EMG signal analysis such as: filtering, wavelet transform, and modeling will be presented in this paper to provide efficient and effective ways of understanding the signal. A comparison study is also given to show performance of various EMG signal analysis methods. This paper provides researchers a good understanding of EMG signal and its analysis procedures. This knowledge will help to develop more flexible and efficient applications.},
	language = {en},
	number = {4},
	author = {Pauk, J},
	year = {2008},
	pages = {7},
	file = {Pauk - 2008 - 419. Different techniques for EMG signal processin.pdf:/home/rommel/Zotero/storage/W67LE2MM/Pauk - 2008 - 419. Different techniques for EMG signal processin.pdf:application/pdf}
}

@article{janke_emg--speech:_2017,
	title = {{EMG}-to-{Speech}: {Direct} {Generation} of {Speech} {From} {Facial} {Electromyographic} {Signals}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	shorttitle = {{EMG}-to-{Speech}},
	url = {http://ieeexplore.ieee.org/document/8114359/},
	doi = {10.1109/TASLP.2017.2738568},
	abstract = {Silent speech interfaces are systems that enable speech communication even when an acoustic signal is unavailable. Over the last years, public interest in such interfaces has intensiﬁed. They provide solutions for some of the challenges faced by today’s speech-driven technologies, such as robustness to noise and usability for people with speech impediments. In this paper, we provide an overview over our silent speech interface. It is based on facial surface electromyography (EMG), which we use to record the electrical signals that control muscle contraction during speech production. These signals are then converted directly to an audible speech waveform, retaining important paralinguistic speech cues for information such as speaker identity and mood. This paper gives an overview over our state-of-the-art direct EMG-to-speech transformation system. This paper describes the characteristics of the speech EMG signal, introduces techniques for extracting relevant features, presents different EMG-to-speech mapping methods, and ﬁnally, presents an evaluation of the different methods for real-time capability and conversion quality.},
	language = {en},
	number = {12},
	urldate = {2019-02-27},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Janke, Matthias and Diener, Lorenz},
	month = dec,
	year = {2017},
	pages = {2375--2385},
	annote = {
used a 3-hidden-layer feed forward neural network
To avoid bias towards numerically larger EMG- or audio features, the signalis normalized to zero mean and unit variance. Dropout [75] isused to reduce overfitting.
Used LSTM - The LSTMs used in this work are bidirectional LSTMs. Graves et al. [80].
According to extensive experiments on electrodepositioning [84], EMG-based speech processing requires, at thevery least, signals from the cheek area and the throat (to capturetongue activity).
All measurements were obtained on an Intel Core i7-2700CPU running at 3.5 GHz.
For the evaluation of LSTM networks, we used theCURRENNT implementation
},
	file = {Janke and Diener - 2017 - EMG-to-Speech Direct Generation of Speech From Fa.pdf:/home/rommel/Zotero/storage/NFCVY6GT/Janke and Diener - 2017 - EMG-to-Speech Direct Generation of Speech From Fa.pdf:application/pdf}
}

@inproceedings{kapur_alterego:_2018,
	address = {Tokyo, Japan},
	title = {{AlterEgo}: {A} {Personalized} {Wearable} {Silent} {Speech} {Interface}},
	isbn = {978-1-4503-4945-1},
	shorttitle = {{AlterEgo}},
	url = {http://dl.acm.org/citation.cfm?doid=3172944.3172977},
	doi = {10.1145/3172944.3172977},
	abstract = {We present a wearable interface that allows a user to silently converse with a computing device without any voice or any discernible movements - thereby enabling the user to communicate with devices, AI assistants, applications or other people in a silent, concealed and seamless manner. A user's intention to speak and internal speech is characterized by neuromuscular signals in internal speech articulators that are captured by the AlterEgo system to reconstruct this speech. We use this to facilitate a natural language user interface, where users can silently communicate in natural language and receive aural output (e.g - bone conduction headphones), thereby enabling a discreet, bi-directional interface with a computing device, and providing a seamless form of intelligence augmentation. The paper describes the architecture, design, implementation and operation of the entire system. We demonstrate robustness of the system through user studies and report 92\% median word accuracy levels.},
	language = {en},
	urldate = {2019-02-27},
	booktitle = {Proceedings of the 2018 {Conference} on {Human} {Information} {Interaction}\&{Retrieval}  - {IUI} '18},
	publisher = {ACM Press},
	author = {Kapur, Arnav and Kapur, Shreyas and Maes, Pattie},
	year = {2018},
	pages = {43--53},
	annote = {
Data was collected during two mainphases. First, we conducted a pilot study with 3 participants(1 female, average age of 29.33 years) to investigatefeasibility of signal detection and to determine electrodepositioning. The preliminary dataset recorded with theparticipants was binary, with the world labels being yes andno.
This feature representation is passed through a 1-dimensionalconvolutional neural network to classify into word labelswith the architecture described as follows. The hidden layerconvolves 400 filters of kernel size 3 with stride 1 with theprocessed input and is then passed through a rectifiernonlinearity. This is subsequently followed by a maxpooling layer.
This unit is repeated twice before globally max poolingover its input. This is followed by a fully connected layer ofdimension 200 passed through a rectifier nonlinearity whichis followed by another fully connected layer with a sigmoidactivation. The network was optimized using a first ordergradient descent and parameters were updated using Adam[19] during training. The network was regularized using a50\% dropout in each hidden layer to enable the network togeneralize better on unseen data.
In order to help the users understand silent speech, weshowed the user a piece of text and asked the user to read itlike (s)he silently read online articles, i.e. read to oneselfand not out loud. For each participant, we showed them atotal of 750 digits, randomly sequenced on a computerscreen, and instructed the users to ‘read the number tothemselves, without producing a sound and moving theirlips’. The digits were randomly chosen from a total of 10digits (0 to 9), such that each digit exactly appeared 75times.
},
	file = {Kapur et al. - 2018 - AlterEgo A Personalized Wearable Silent Speech In.pdf:/home/rommel/Zotero/storage/P77QZRTP/Kapur et al. - 2018 - AlterEgo A Personalized Wearable Silent Speech In.pdf:application/pdf}
}

@misc{noauthor_interspeech_nodate,
	title = {Interspeech 2007 {Abstract}: {Wand} et al.},
	url = {https://www.isca-speech.org/archive/interspeech_2007/i07_0686.html},
	urldate = {2019-03-07},
	annote = {
Non-audible speech is favored for the sake of privacy,for example when making a confidential phone call in publicspaces. Last but not least, alternative input methods for speechrecognition may be useful for patients with medical speech impairments.
 
},
	file = {wand_is07.pdf:/home/rommel/Zotero/storage/XSS47XFD/wand_is07.pdf:application/pdf}
}

@misc{martti_vanino_continuous_2017,
	address = {HSE Moscow},
	title = {Continuous {Wavelet} {Transform} for {Speech} {Research}},
	url = {https://ilcl.hse.ru/data/2017/12/08/1161391021/Martti%20Vainio%20-%20Continuous%20wavelet%20transform%20for%20speech%20research.pdf},
	language = {English},
	author = {{Martti Vanino} and {Juraj Simko} and {Antti Suni}},
	month = nov,
	year = {2017}
}

@article{eltvik_deep_nodate,
	title = {Deep {Learning} for the {Classification} of {EEG} {Time}-{Frequency} {Representations}},
	abstract = {This thesis is a report on the implementation and evaluation of a new method classifying EEG signals. The method involves applying either the Short-time Fourier Transform (STFT), Continuous Wavelet Transform (CWT) or Hilbert-Huang Transform (HHT) to produce a two-dimensional time-frequency representation of the signal, known as spectrograms, scalograms and Hilbert spectra, respectively. These two-dimensional representations are then classiﬁed using a Convolutional Neural Network (CNN).},
	language = {en},
	author = {Eltvik, Audun},
	pages = {122},
	file = {Eltvik - Deep Learning for the Classification of EEG Time-F.pdf:/home/rommel/Zotero/storage/V94ZW3GC/Eltvik - Deep Learning for the Classification of EEG Time-F.pdf:application/pdf}
}

@article{altamirano_emg_nodate,
	title = {{EMG} {Pattern} {Prediction} for {Upper} {Limb} {Movements} {Based} on {Wavelet} and {Hilbert}-{Huang} {Transform}},
	language = {en},
	author = {Altamirano, Alvaro Altamirano},
	pages = {134},
	file = {Altamirano - EMG Pattern Prediction for Upper Limb Movements Ba.pdf:/home/rommel/Zotero/storage/6EFZ6NEC/Altamirano - EMG Pattern Prediction for Upper Limb Movements Ba.pdf:application/pdf}
}

@misc{noauthor_classify_nodate,
	title = {Classify {Time} {Series} {Using} {Wavelet} {Analysis} and {Deep} {Learning} - {MATLAB} \& {Simulink} {Example}},
	url = {https://www.mathworks.com/help/wavelet/examples/signal-classification-with-wavelet-analysis-and-convolutional-neural-networks.html},
	urldate = {2019-03-13},
	file = {Classify Time Series Using Wavelet Analysis and Deep Learning - MATLAB & Simulink Example:/home/rommel/Zotero/storage/8ZLZTC4V/signal-classification-with-wavelet-analysis-and-convolutional-neural-networks.html:text/html}
}

@inproceedings{maier-hein_session_2005,
	address = {San Juan, Puerto Rico},
	title = {Session independent non-audible speech recognition using surface electromyography},
	isbn = {978-0-7803-9479-7},
	url = {http://ieeexplore.ieee.org/document/1566521/},
	doi = {10.1109/ASRU.2005.1566521},
	abstract = {In this paper we introduce a speech recognition system based on myoelectric signals. The system handles audible and non-audible speech. Major challenges in surface electromyography based speech recognition ensue from repositioning electrodes between recording sessions, environmental temperature changes, and skin tissue properties of the speaker. In order to reduce the impact of these factors, we investigate a variety of signal normalization and model adaptation methods. An average word accuracy of 97.3\% is achieved using seven EMG channels and the same electrode positions. The performance drops to 76.2\% after repositioning the electrodes if no normalization or adaptation is performed. By applying our adaptation methods we manage to restore the recognition rates to 87.1\%. Furthermore, we compare audibly to non-audibly spoken speech. The results suggest that large differences exist between the corresponding muscle movements. Still, our recognition system recognizes both speech manners accurately when trained on pooled data.},
	language = {en},
	urldate = {2019-03-14},
	booktitle = {{IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding}, 2005.},
	publisher = {IEEE},
	author = {Maier-Hein, L. and Metze, F. and Schultz, T. and Waibel, A.},
	year = {2005},
	pages = {331--336},
	annote = {
A high-pass filteris applied to avoid aliasing artefacts whereasalow-pass filteris used to reduce movement artefacts in the sig-nals
in this study, isolated word recognition was performed on a vocab-ulary consisting of the ten English digits “zero” to “nine”.
Figure 1 the electrodes were positioned such that theyobtain the EMG signal of six articular muscles: thelevator angulioris(EMG2,3), thezygomaticus major(EMG2,3), theplatysma(EMG4,5) thedepressor anguli oris(EMG5), theanterior bellyof thedigastric(EMG1) and thetongue(EMG1,6,7) [10, 6]. Forthree of the seven EMG channels (EMG2,6,7) a classical bipolarelectrode configuration with a 2cm center-to-center inter-electrodespacing was used. For the remaining four channels one of thedetection electrodes was placed directly on the articulatory mus-cles and was referenced to either the nose (EMG1) or to both ears(EMG3,4,5) (Figure 1). The positioning of the electrodes was op-timized in previous experiments, not reported here.
},
	file = {Maier-Hein et al. - 2005 - Session independent non-audible speech recognition.pdf:/home/rommel/Zotero/storage/W4PBGVBU/Maier-Hein et al. - 2005 - Session independent non-audible speech recognition.pdf:application/pdf}
}

@inproceedings{diener_direct_2015,
	title = {Direct conversion from facial myoelectric signals to speech using {Deep} {Neural} {Networks}},
	doi = {10.1109/IJCNN.2015.7280404},
	abstract = {This paper presents our first results using Deep Neural Networks for surface electromyographic (EMG) speech synthesis. The proposed approach enables a direct mapping from EMG signals captured from the articulatory muscle movements to the acoustic speech signal. Features are processed from multiple EMG channels and are fed into a feed forward neural network to achieve a mapping to the target acoustic speech output. We show that this approach is feasible to generate speech output from the input EMG signal and compare the results to a prior mapping technique based on Gaussian mixture models. The comparison is conducted via objective Mel-Cepstral distortion scores and subjective listening test evaluations. It shows that the proposed Deep Neural Network approach gives substantial improvements for both evaluation criteria.},
	booktitle = {2015 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Diener, L. and Janke, M. and Schultz, T.},
	month = jul,
	year = {2015},
	keywords = {feature extraction, electromyography, Electromyography, medical signal processing, acoustic speech signal, articulatory muscle movements, cepstral analysis, deep neural network approach, EMG signal capture, facial myoelectric signal-to-speech conversion, feature processing, feedforward neural network, Gaussian mixture models, Gaussian processes, mixture models, multiple EMG channels, neural nets, neurophysiology, objective Mel-Cepstral distortion scores, prior mapping technique, speech, subjective listening test evaluations, surface electromyographic speech synthesis},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:/home/rommel/Zotero/storage/MPVERZJC/7280404.html:text/html;IEEE Xplore Full Text PDF:/home/rommel/Zotero/storage/TGKN9SVR/Diener et al. - 2015 - Direct conversion from facial myoelectric signals .pdf:application/pdf}
}

@article{higham_deep_2018,
	title = {Deep {Learning}: {An} {Introduction} for {Applied} {Mathematicians}},
	shorttitle = {Deep {Learning}},
	url = {http://arxiv.org/abs/1801.05894},
	abstract = {Multilayered artiﬁcial neural networks are becoming a pervasive tool in a host of application ﬁelds. At the heart of this deep learning revolution are familiar concepts from applied and computational mathematics; notably, in calculus, approximation theory, optimization and linear algebra. This article provides a very brief introduction to the basic ideas that underlie deep learning from an applied mathematics perspective. Our target audience includes postgraduate and ﬁnal year undergraduate students in mathematics who are keen to learn about the area. The article may also be useful for instructors in mathematics who wish to enliven their classes with references to the application of deep learning techniques. We focus on three fundamental questions: what is a deep neural network? how is a network trained? what is the stochastic gradient method? We illustrate the ideas with a short MATLAB code that sets up and trains a network. We also show the use of state-of-the art software on a large scale image classiﬁcation problem. We ﬁnish with references to the current literature.},
	language = {en},
	urldate = {2019-03-15},
	journal = {arXiv:1801.05894 [cs, math, stat]},
	author = {Higham, Catherine F. and Higham, Desmond J.},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.05894},
	keywords = {97R40, 68T01, 65K10, 62M45, Computer Science - Machine Learning, G.1.6, I.2.0, I.2.10, I.2.6, Mathematics - History and Overview, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	file = {Higham and Higham - 2018 - Deep Learning An Introduction for Applied Mathema.pdf:/home/rommel/Zotero/storage/CPJ9Y3KZ/Higham and Higham - 2018 - Deep Learning An Introduction for Applied Mathema.pdf:application/pdf}
}

@article{stepp_surface_2012,
	title = {Surface {Electromyography} for {Speech} and {Swallowing} {Systems}: {Measurement}, {Analysis}, and {Interpretation}},
	volume = {55},
	issn = {1092-4388, 1558-9102},
	shorttitle = {Surface {Electromyography} for {Speech} and {Swallowing} {Systems}},
	url = {http://pubs.asha.org/doi/10.1044/1092-4388%282011/11-0214%29},
	doi = {10.1044/1092-4388(2011/11-0214)},
	abstract = {Purpose: Applying surface electromyography (sEMG) to the study of voice, speech, and swallowing is becoming increasingly popular. An improved understanding of sEMG and building a consensus as to appropriate methodology will improve future research and clinical applications. Method: An updated review of the theory behind recording sEMG for the speech and swallowing systems is provided. Several factors that are known to affect the content of the sEMG signal are discussed, and practical guidelines for sEMG recording and analysis are presented, focusing on special considerations within the context of the speech and swallowing anatomy. Results: Unique challenges are seen in application of sEMG to the speech and swallowing musculature owing to the small size of the muscles in relation to the sEMG detection volume and the present lack of knowledge about innervation zone locations. Conclusions: Despite the challenges discussed, application of sEMG to speech and swallowing has potential as a clinical and research tool when used correctly and is specifically suited to noninvasive clinical studies using between-condition or between-group comparisons for which detection of specific isolated muscle activities is not necessary.},
	language = {en},
	number = {4},
	urldate = {2019-03-18},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Stepp, Cara E.},
	month = aug,
	year = {2012},
	pages = {1232--1246},
	annote = {Diagram of neck muscles as seen from the front illustratingexamples of bipolar electrode configurations. A: Incorrect bilateralconfiguration. B: Suggested configuration with electrodes placedparallel to the longitudinal axis of the muscle body, in line with thefibers of the muscle. TH = thyrohyoid; OH = omohyoid, SCM =sternocleidomastoid; SH = sternohyoid; ST = sternothyroid},
	file = {Stepp - 2012 - Surface Electromyography for Speech and Swallowing.pdf:/home/rommel/Zotero/storage/8ANI4BZU/Stepp - 2012 - Surface Electromyography for Speech and Swallowing.pdf:application/pdf}
}

@misc{lukas_masuch_deep_nodate,
	type = {Technology},
	title = {Deep {Learning} - {The} {Past}, {Present} and {Future} of {Artificial} {Intelligen}…},
	url = {https://www.slideshare.net/LuMa921/deep-learning-the-past-present-and-future-of-artificial-intelligence?ref=https://www.analyticsindiamag.com/popular-presentations-on-artificial-intelligence-and-machine-learning/},
	abstract = {In the last couple of years, deep learning techniques have transformed the},
	urldate = {2019-03-20},
	author = {Lukas Masuch}
}

@misc{noauthor_classify_nodate-1,
	title = {Classify {ECG} {Signals} {Using} {LSTM} {Networks}},
	url = {https://blogs.mathworks.com/deep-learning/2018/08/06/classify-ecg-signals-using-lstm-networks/},
	abstract = {Today I want to highlight a signal processing application of deep learning. This example, which is from the Signal Processing Toolbox documentation, shows how to classify heartbeat electrocardiogram (ECG) data from the PhysioNet 2017 Challenge using deep learning and signal processing. In particular, the example uses Long Short-Term Memory (LSTM) networks and time-frequency analysis. This example requires Neural Network Toolbox™.


Contents},
	urldate = {2019-03-23},
	journal = {Deep Learning},
	file = {Snapshot:/home/rommel/Zotero/storage/GUA9H26Y/classify-ecg-signals-using-lstm-networks.html:text/html}
}

@misc{nies_diagnosing_2018,
	title = {Diagnosing {Myocardial} {Infarction} using {Long}-{Short} {Term} {Memory} networks ({LSTM}’s)},
	url = {https://blog.orikami.nl/diagnosing-myocardial-infarction-using-long-short-term-memory-networks-lstms-cedf5770a257},
	abstract = {Introduction},
	urldate = {2019-03-23},
	journal = {Orikami blog},
	author = {Nies, Luc},
	month = jul,
	year = {2018},
	file = {Snapshot:/home/rommel/Zotero/storage/YFWZDEV8/diagnosing-myocardial-infarction-using-long-short-term-memory-networks-lstms-cedf5770a257.html:text/html}
}

@misc{noauthor_how_nodate,
	title = {How to {Create} a {Simple} {Low}-{Pass} {Filter} - {TomRoelandts}.com},
	url = {https://tomroelandts.com/articles/how-to-create-a-simple-low-pass-filter},
	urldate = {2019-04-07},
	file = {How to Create a Simple Low-Pass Filter - TomRoelandts.com:/home/rommel/Zotero/storage/2RE7GCP8/how-to-create-a-simple-low-pass-filter.html:text/html}
}

@misc{admin_guide_2018,
	title = {A guide for using the {Wavelet} {Transform} in {Machine} {Learning}},
	url = {http://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/},
	abstract = {[latexpage] 1. Introduction In a previous blog-post we have seen how we can use Signal Processing techniques for the classification of time-series and signals. A very short summary of that post is:…},
	language = {nl},
	urldate = {2019-04-08},
	journal = {Ahmet Taspinar},
	author = {{admin}},
	month = dec,
	year = {2018},
	file = {Snapshot:/home/rommel/Zotero/storage/2GRIV2TH/admin - 2018 - A guide for using the Wavelet Transform in Machine.html:text/html}
}

@article{liu_electromyographic_nodate,
	title = {Electromyographic {Signal} {Processing} {With} {Application} {To} {Spinal} {Cord} {Injury}},
	abstract = {An Electromyogram or Electromyographic (EMG) signal is the recording of the electrical activity produced by muscles. It measures the electric currents generated in muscles during their contraction. The EMG signal provides insight into the neural activation and dynamics of the muscles, and is therefore important for many diﬀerent applications, such as in clinical investigations that attempt to diagnose neuromuscular deﬁciencies. In particular, the work in this thesis is motivated by rehabilitation for patients with spinal cord injury. The EMG signal is very important for researchers and practitioners to monitor and evaluate the eﬀect of the rehabilitation training and the condition of muscles, as the EMG signal provides information that helps infer the neural activity in the spinal cord. Before the work in this thesis, EMG analysis required signiﬁcant amounts of manual labeling of interesting signal features. The motivation of this thesis is to fully automate the EMG analysis tasks and yield accurate, consistent results.},
	language = {en},
	author = {Liu, Zhao},
	pages = {143},
	file = {Liu - Electromyographic Signal Processing With Applicati.pdf:/home/rommel/Zotero/storage/ZJUDVFT3/Liu - Electromyographic Signal Processing With Applicati.pdf:application/pdf}
}

@misc{noauthor_understanding_nodate,
	title = {Understanding {LSTM} {Networks} -- colah's blog},
	url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2019-04-14},
	file = {Understanding LSTM Networks -- colah's blog:/home/rommel/Zotero/storage/2XIG2YUX/2015-08-Understanding-LSTMs.html:text/html}
}

@misc{noauthor_cs231n_nodate,
	title = {{CS}231n {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {http://cs231n.github.io/convolutional-networks/},
	urldate = {2019-04-14},
	file = {CS231n Convolutional Neural Networks for Visual Recognition:/home/rommel/Zotero/storage/RM79DDLI/convolutional-networks.html:text/html}
}